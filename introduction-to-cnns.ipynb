{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install --quiet \"tabulate\" \"ipython[notebook]>=8.0.0, <8.12.0\" \"pytorch-lightning>=1.4, <2.0.0\" \"torchvision\" \"setuptools==67.4.0\" \"seaborn\" \"torch>=1.8.1, <1.14.0\" \"matplotlib\" \"torchmetrics>=0.7, <0.12\" \"lightning>=2.0.0rc0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-14T07:07:23.563391Z","iopub.execute_input":"2023-09-14T07:07:23.563770Z","iopub.status.idle":"2023-09-14T07:09:56.090821Z","shell.execute_reply.started":"2023-09-14T07:07:23.563735Z","shell.execute_reply":"2023-09-14T07:09:56.089175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Why CNN'S\n* ANN'S are not designed to process data like Images which have spatial information because we would need to flatten them which leads to a loss of spatial information\n* CNN are equivariant meaning that the resultant feature map obtained by first augmenting and then performing convolution is the same as the one obtained by first performing convolution and then performing augmentation making them invariant to spatial transformation\n\n### How they work\n* Feature extractor - this component is responsible for performing convolution on images and extracting spatial information(edges, contour, eyes, nose)\n    * Convolution Layer\n        * Performs the convolution operation on the image to extract spatial information utilizing\n            * Kernel - filter supposed to perform convolution on images. It is first randomly initialized but later updated through back propagation to extract the spatial features more accurately. We then do element-wise product and sum of the filter matrix and the original image\n            * Stride - Number of pixels by which the kernel moves while performing convolution\n            * Padding - Number of pixels by which the image is padded while performing convolution sice convolution on an image leads to a reduction in the image size. So we pad to preserve the image size\n            * Kernel size - This is the size of the kernel which is used to perform the convolution on Images. Using a larger kernel size leads to faster training but loss of details in the image\n            * Number of filters - this is the number of convolution filters which are convoluted on the images. Each filter is stacked along the channels\n        \n    * Activation Function\n        * introduces non-linearity in the feature maps. This is done to enhance the learning process enabling the nn to learn more complex patterns\n \n    * Pooling Layer\n        * Pooling is donne to reduce the number of params that gradually increase due to repeated convolutions while preserving maximum information\n        * Achieved by sliding a window(usually 2* 2 window) over the resultant feature map and picks up the pooled pixel in each window\n        * Variants include:\n            * Max Pooling - we pick up the pixel which has the maximum value in each pixel. Throws away all other non useful info\n            * Min Pooling - We pick up the pixel which has the minimum value in each pooling window\n            * Average Pooling - We average all the pixels in each pooling window. Retains much info about the less important elements of a block or pool \n        \n* Flattening - this component is responsible for converting the multi dimensional feature map to a 1D array which can later be processed by the classification head\n* Classification Head - this component is composed of several fully connected layers which perform classification\n\n\n### How do CNNs Learn\n* They preserve the spatial information of the image by extracting the feature maps. By performing convolution using multiple filters the CNN learns a variety of spatial information like edges, shapes, objects e.t.c which are then flattened and passed to the classification head to predict the classes or the regression head to predict the desired quantity\n    * Convolution + ReLU\n    * max_pooling\n    * fully_connected + ReLU\n    * softmax\n\n","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size):\n        super().__init__()\n        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size)\n        self.pooling_layer = nn.MaxPool2d()\n        \n    def forward(self, x):\n        x = self.conv_layer(x)\n        x = nn.ReLU()(x)\n        x = self.pooling_layer(x)\n        return x\n    \n    \nclass MyCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #Assuming input shape = (224,224,3) and padding = 0\n        self.feature_extractor = nn.Sequential(\n            ConvBlock(3, 64, (3,3)), #takes in an image of rgb, produces 64 output channels feature maps, uses a 3 by 3 filter kernel \n            ConvBlock(64, 128, (3,3)),#takes in 64 feature maps from above produces 128 output channels uses a 3 by 3 filter kernel\n            ConvBlock(128, 256, (3,3)),#takes in 128 input channels produces 256 output channles uses a 3 by 3 filter kernel\n            ConvBlock(256, 512, (3,3)) # takes in 256 input channels produces 512 output channels uses a 3 by 3 filter kernel - 12,12,512\n        )\n        self.flatten = nn.Flatten() #12 * 12 * 512\n        self.classification_head = nn.Sequential(\n            nn.Linear(in_features = 12*12*512, out_features =64),\n            nn.ReLU().\n            nn.Linear(in_features = 64, out_features = 32),\n            nn.ReLU(),\n            nn.Linear(in_features = 32, out_features = num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.feature_extractor(x)\n        x = self.flatten(x)\n        x = self.classification_head(x)\n        return x\n        \n        \n    \n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:25:30.269429Z","iopub.execute_input":"2023-09-14T08:25:30.269842Z","iopub.status.idle":"2023-09-14T08:25:30.284067Z","shell.execute_reply.started":"2023-09-14T08:25:30.269810Z","shell.execute_reply":"2023-09-14T08:25:30.282774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loss Functions in CNNS\n* Image classification\n    * Hinge Loss\n    * Cross Entropy Loss\n    \n* Image regression:\n    * MSE\n    * MAE\n    * Huber Loss\n    \n### Tricks to speed up training\n* Use batch normalization\n    * speeds up training\n    * allows for a large learning rate\n    * does not depend on weight initialization\n    \n* Use Global Average Pooling instead of Flatten to convert output to 1D \n    * captures the information of the feature map in each channel by taking the average\n    * Reduces the number of params required to train\n    * So from (channels, width, height) the average is taken along each chaneel, so the resultant shape becomes (channels, 1,1) which can be reshaped into (channels, 1) and a classification head can be attached afterwards\n   ","metadata":{}},{"cell_type":"markdown","source":"### Lets build a CNN network","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:48:22.293255Z","iopub.execute_input":"2023-09-14T08:48:22.293711Z","iopub.status.idle":"2023-09-14T08:48:22.574395Z","shell.execute_reply.started":"2023-09-14T08:48:22.293679Z","shell.execute_reply":"2023-09-14T08:48:22.573137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Setup and Data Preparation","metadata":{}},{"cell_type":"code","source":"n_epochs = 3\nbatch_size_train = 64\nbatch_size_test = 1000\nlearning_rate = 0.01\nmomentum = 0.5\nlog_interval = 10\n\nrandom_seed = 1\ntorch.backends.cudnn.enabled = False\ntorch.manual_seed(random_seed)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:48:46.775609Z","iopub.execute_input":"2023-09-14T08:48:46.776017Z","iopub.status.idle":"2023-09-14T08:48:46.790539Z","shell.execute_reply.started":"2023-09-14T08:48:46.775985Z","shell.execute_reply":"2023-09-14T08:48:46.789301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(\n  torchvision.datasets.MNIST('/files/', train=True, download=True,\n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize(\n                                 (0.1307,), (0.3081,))\n                             ])),\n  batch_size=batch_size_train, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(\n  torchvision.datasets.MNIST('/files/', train=False, download=True,\n                             transform=torchvision.transforms.Compose([\n                               torchvision.transforms.ToTensor(),\n                               torchvision.transforms.Normalize(\n                                 (0.1307,), (0.3081,))\n                             ])),\n  batch_size=batch_size_test, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:49:03.473390Z","iopub.execute_input":"2023-09-14T08:49:03.473827Z","iopub.status.idle":"2023-09-14T08:49:04.852879Z","shell.execute_reply.started":"2023-09-14T08:49:03.473792Z","shell.execute_reply":"2023-09-14T08:49:04.851515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"examples = enumerate(test_loader)\nbatch_idx, (example_data, example_targets) = next(examples)\nexample_data.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:50:00.308691Z","iopub.execute_input":"2023-09-14T08:50:00.309141Z","iopub.status.idle":"2023-09-14T08:50:00.566584Z","shell.execute_reply.started":"2023-09-14T08:50:00.309105Z","shell.execute_reply":"2023-09-14T08:50:00.565503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig = plt.figure()\nfor i in range(6):\n  plt.subplot(2,3,i+1)\n  plt.tight_layout()\n  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n  plt.xticks([])\n  plt.yticks([])\nfig","metadata":{"execution":{"iopub.status.busy":"2023-09-14T08:50:14.636743Z","iopub.execute_input":"2023-09-14T08:50:14.637280Z","iopub.status.idle":"2023-09-14T08:50:15.757820Z","shell.execute_reply.started":"2023-09-14T08:50:14.637234Z","shell.execute_reply":"2023-09-14T08:50:15.756604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Building the Network","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn #torch.nn layers contain trainable params\nimport torch.nn.functional as F#these ones are purely functional\nimport torch.optim as optim\n\n#For readability\nclass MyCNN(nn.Module):\n    def __init__(self):\n        super(MyCNN, self).__init__()\n        \n        self.feature_extractor = nn.Sequential(\n            nn.Conv2d(1,10, kernel_size = 5),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            \n            nn.Conv2d(10, 20, kernel_size = 5),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n            \n        \n        )\n        self.flatten = nn.Flatten()\n        \n        self.classification_head = nn.Sequential(\n            nn.Linear(320, 50),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(50, 10)\n        )\n    def forward(self, x):\n        #pass it to the feature extractor\n        x = self.feature_extractor(x)\n        #Flatten the output\n        x = self.flatten(x)\n        \n        #forward pass through the classification layers\n        x = self.classification_head(x)\n        \n        return F.log_softmax(x)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:17:14.911340Z","iopub.execute_input":"2023-09-14T09:17:14.911733Z","iopub.status.idle":"2023-09-14T09:17:14.923574Z","shell.execute_reply.started":"2023-09-14T09:17:14.911702Z","shell.execute_reply":"2023-09-14T09:17:14.922477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize the network and the optimizer","metadata":{}},{"cell_type":"code","source":"network = MyCNN()\noptimizer = optim.SGD(network.parameters(),lr = learning_rate, momentum = momentum)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:17:54.684134Z","iopub.execute_input":"2023-09-14T09:17:54.684576Z","iopub.status.idle":"2023-09-14T09:17:54.692332Z","shell.execute_reply.started":"2023-09-14T09:17:54.684542Z","shell.execute_reply":"2023-09-14T09:17:54.691118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Train the Network","metadata":{}},{"cell_type":"code","source":"train_losses = []\ntrain_counter = []\ntest_losses = []\ntest_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n\ndef train(epoch):\n  network.train()\n  for batch_idx, (data, target) in enumerate(train_loader):\n    optimizer.zero_grad()\n    output = network(data)\n    loss = F.nll_loss(output, target)\n    loss.backward()\n    optimizer.step()\n    if batch_idx % log_interval == 0:\n      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n        epoch, batch_idx * len(data), len(train_loader.dataset),\n        100. * batch_idx / len(train_loader), loss.item()))\n      train_losses.append(loss.item())\n      train_counter.append(\n        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n      torch.save(network.state_dict(), 'model.pth')\n      torch.save(optimizer.state_dict(), 'optimizer.pth')\n    \ndef test():\n  network.eval()\n  test_loss = 0\n  correct = 0\n  with torch.no_grad():\n    for data, target in test_loader:\n      output = network(data)\n      test_loss += F.nll_loss(output, target, size_average=False).item()\n      pred = output.data.max(1, keepdim=True)[1]\n      correct += pred.eq(target.data.view_as(pred)).sum()\n  test_loss /= len(test_loader.dataset)\n  test_losses.append(test_loss)\n  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n    test_loss, correct, len(test_loader.dataset),\n    100. * correct / len(test_loader.dataset)))           \n            \n            ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:29:06.673562Z","iopub.execute_input":"2023-09-14T09:29:06.674021Z","iopub.status.idle":"2023-09-14T09:29:06.689612Z","shell.execute_reply.started":"2023-09-14T09:29:06.673984Z","shell.execute_reply":"2023-09-14T09:29:06.688565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test()\nfor epoch in range(1, n_epochs + 1):\n  train(epoch)\n  test()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:29:08.637093Z","iopub.execute_input":"2023-09-14T09:29:08.637975Z","iopub.status.idle":"2023-09-14T09:30:39.966508Z","shell.execute_reply.started":"2023-09-14T09:29:08.637928Z","shell.execute_reply":"2023-09-14T09:30:39.965122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating the Model's Performance","metadata":{}},{"cell_type":"code","source":"fig = plt.figure()\nplt.plot(train_counter, train_losses, color='blue')\nplt.scatter(test_counter, test_losses, color='red')\nplt.legend(['Train Loss', 'Test Loss'], loc='upper right')\nplt.xlabel('number of training examples seen')\nplt.ylabel('negative log likelihood loss')\nfig","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:30:39.969102Z","iopub.execute_input":"2023-09-14T09:30:39.970282Z","iopub.status.idle":"2023-09-14T09:30:40.595261Z","shell.execute_reply.started":"2023-09-14T09:30:39.970233Z","shell.execute_reply":"2023-09-14T09:30:40.594125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Sample Prediction on our example dataset","metadata":{}},{"cell_type":"code","source":"with torch.no_grad():\n    output = network(example_data)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:30:40.596593Z","iopub.execute_input":"2023-09-14T09:30:40.596904Z","iopub.status.idle":"2023-09-14T09:30:40.694603Z","shell.execute_reply.started":"2023-09-14T09:30:40.596877Z","shell.execute_reply":"2023-09-14T09:30:40.693314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nfor i in range(6):\n  plt.subplot(2,3,i+1)\n  plt.tight_layout()\n  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n  plt.title(\"Prediction: {}\".format(\n    output.data.max(1, keepdim=True)[1][i].item()))\n  plt.xticks([])\n  plt.yticks([])\nfig","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:30:40.696663Z","iopub.execute_input":"2023-09-14T09:30:40.697102Z","iopub.status.idle":"2023-09-14T09:30:41.727738Z","shell.execute_reply.started":"2023-09-14T09:30:40.697071Z","shell.execute_reply":"2023-09-14T09:30:41.726132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Load model from checkpoint and COntinue training your model further","metadata":{}},{"cell_type":"code","source":"continued_network = MyCNN()\ncontinued_optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n                                momentum=momentum)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:31:20.575186Z","iopub.execute_input":"2023-09-14T09:31:20.575623Z","iopub.status.idle":"2023-09-14T09:31:20.583909Z","shell.execute_reply.started":"2023-09-14T09:31:20.575587Z","shell.execute_reply":"2023-09-14T09:31:20.582631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network_state_dict = torch.load(\"/kaggle/working/model.pth\")\ncontinued_network.load_state_dict(network_state_dict)\n\noptimizer_state_dict = torch.load(\"/kaggle/working/optimizer.pth\")\ncontinued_optimizer.load_state_dict(optimizer_state_dict)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:32:14.761926Z","iopub.execute_input":"2023-09-14T09:32:14.762367Z","iopub.status.idle":"2023-09-14T09:32:14.775457Z","shell.execute_reply.started":"2023-09-14T09:32:14.762334Z","shell.execute_reply":"2023-09-14T09:32:14.774242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(4,9):\n  test_counter.append(i*len(train_loader.dataset))\n  train(i)\n  test()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:32:22.954915Z","iopub.execute_input":"2023-09-14T09:32:22.955357Z","iopub.status.idle":"2023-09-14T09:34:53.326843Z","shell.execute_reply.started":"2023-09-14T09:32:22.955322Z","shell.execute_reply":"2023-09-14T09:34:53.325477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure()\nplt.plot(train_counter, train_losses, color='blue')\nplt.scatter(test_counter, test_losses, color='red')\nplt.legend(['Train Loss', 'Test Loss'], loc='upper right')\nplt.xlabel('number of training examples seen')\nplt.ylabel('negative log likelihood loss')\nfig","metadata":{"execution":{"iopub.status.busy":"2023-09-14T09:34:53.329682Z","iopub.execute_input":"2023-09-14T09:34:53.330677Z","iopub.status.idle":"2023-09-14T09:34:53.903431Z","shell.execute_reply.started":"2023-09-14T09:34:53.330626Z","shell.execute_reply":"2023-09-14T09:34:53.902159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}