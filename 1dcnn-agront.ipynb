{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unveiling Cassava's Secrets\n* AI Hackathon for Accelerating African Agrigenomics\n* Goal of these notebook is to we will leverage experimentally mapped cassava enhancers and InstaDeep’s AgroNT model to build predictors for enhancer regulatory activity in the cassava genome.\n* I will use a 1D CNN model\n* Credits: https://www.kaggle.com/code/yasufuminakama/moa-pytorch-nn-starter/notebook","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Getting started with the Data\n* Make sure you are using GPU\n\nCassava (Manihot esculenta) is the main food staple in Ghana, crucial for sustenance and income for many families. It makes up 22% of Ghana's Agricultural Gross Domestic Product (AGDP) and with an impressive per capita consumption rate of 152 kg, it is also responsible for 30% of the average daily calorie intake (Acheampong et al., 2021). Cassava is a prime example of an ‘orphan crop’, i.e. crops which have been under-researched in comparison to worldwide cash crops such as maize or rice (Yaqoob et al., 2023). One of cassava’s most intriguing secrets is its resistance to drought (Yaqoob et al., 2023), which can help the world face the challenges of climate change and a growing population.\n\n\nIn changing environmental conditions, such as exposure to drought, an organism responds by activating specific genes. Key regulators of gene activity are parts of DNA called ‘enhancers’ - specific sections of DNA that act like power boosters to increase gene expression. Standard genomics methods struggle to identify enhancers accurately because their activity is highly dependent on the context around them and they can be very far from their target genes (Wang et al., 2020).","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedGroupKFold\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:03:59.394313Z","iopub.execute_input":"2023-09-14T13:03:59.394792Z","iopub.status.idle":"2023-09-14T13:04:03.552512Z","shell.execute_reply.started":"2023-09-14T13:03:59.394746Z","shell.execute_reply":"2023-09-14T13:04:03.551544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:03.554335Z","iopub.execute_input":"2023-09-14T13:04:03.554909Z","iopub.status.idle":"2023-09-14T13:04:03.566409Z","shell.execute_reply.started":"2023-09-14T13:04:03.554874Z","shell.execute_reply":"2023-09-14T13:04:03.565434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/indabax-protein-sequence/\"\ntrain = pd.read_csv(path + \"Train(4).csv\")\ntest = pd.read_csv(path + \"Test(4).csv\")\n#load the embeddings from AgroNt that were provided\ntrain_embeds = np.load(path + \"Train_embeddings.npy\")\ntest_embeds = np.load(path + \"Test_embeddings.npy\")\n\n# Create a DataFrame with the embeddings\nembeds_train = pd.DataFrame(train_embeds, columns=[f'embed_{i}' for i in range(train_embeds.shape[1])])\n\n# Create a DataFrame with the test embeddings\nembeds_test = pd.DataFrame(test_embeds, columns=[f'embed_{i}' for i in range(test_embeds.shape[1])])\n\n\n# Concatenate the embeddings DataFrame with your data_df\ntrain = pd.concat([train, embeds_train], axis=1)\n# Concatenate the test embeddings DataFrame with your test_df\ntest= pd.concat([test, embeds_test], axis=1)\ndisplay(train.head(), test.head())","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:03.568023Z","iopub.execute_input":"2023-09-14T13:04:03.568385Z","iopub.status.idle":"2023-09-14T13:04:05.439027Z","shell.execute_reply.started":"2023-09-14T13:04:03.568331Z","shell.execute_reply":"2023-09-14T13:04:05.438118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_cols = test.columns[4:].to_list()\ntarget_cols = ['Target']\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 100\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 12\nEARLY_STOPPING_STEPS = 30\nEARLY_STOP = False\n\n\nnum_targets=train['Target'].nunique()\nhidden_size=4096*4","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:05.444779Z","iopub.execute_input":"2023-09-14T13:04:05.447326Z","iopub.status.idle":"2023-09-14T13:04:05.488500Z","shell.execute_reply.started":"2023-09-14T13:04:05.447289Z","shell.execute_reply":"2023-09-14T13:04:05.487211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GroupKFold","metadata":{}},{"cell_type":"code","source":"gkf = StratifiedGroupKFold(n_splits=12)\n\nfor f, (t_idx, v_idx) in enumerate(gkf.split(train, train['Target'], groups = train['Chromosome'])):\n    train.loc[v_idx, 'fold'] = int(f)\n\ntrain['fold'] = train['fold'].astype(int)\ntrain['fold'].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:05.492488Z","iopub.execute_input":"2023-09-14T13:04:05.493083Z","iopub.status.idle":"2023-09-14T13:04:05.645472Z","shell.execute_reply.started":"2023-09-14T13:04:05.493048Z","shell.execute_reply":"2023-09-14T13:04:05.644523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Introducing PCA\n* To reduce the dimensionality reduction and also trying to introduce some spatial relationship","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA()\nX_new = pca.fit_transform(train[feature_cols])\nexplained_variance=pca.explained_variance_ratio_\nexplained_variance","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:05.647024Z","iopub.execute_input":"2023-09-14T13:04:05.647661Z","iopub.status.idle":"2023-09-14T13:04:11.047930Z","shell.execute_reply.started":"2023-09-14T13:04:05.647627Z","shell.execute_reply":"2023-09-14T13:04:11.045918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(explained_variance)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:11.049321Z","iopub.execute_input":"2023-09-14T13:04:11.049675Z","iopub.status.idle":"2023-09-14T13:04:11.059292Z","shell.execute_reply.started":"2023-09-14T13:04:11.049642Z","shell.execute_reply":"2023-09-14T13:04:11.058087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming you have an array explained_variance with 1500 elements\n# Slice it to get the top 100\ntop_100_explained_variance = explained_variance[:60]\n\nwith plt.style.context('dark_background'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(60), top_100_explained_variance, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:11.060265Z","iopub.execute_input":"2023-09-14T13:04:11.061028Z","iopub.status.idle":"2023-09-14T13:04:11.554429Z","shell.execute_reply.started":"2023-09-14T13:04:11.060996Z","shell.execute_reply":"2023-09-14T13:04:11.553492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Initialize the PCA model with the desired number of components (60 in this case)\npca = PCA(n_components=60)\n\n# Fit the PCA model to your selected columns\npca.fit(train[feature_cols])\n\n# Transform the original data into the new principal components\ntrain_pca_result = pca.transform(train[feature_cols])\ntest_pca_result = pca.transform(test[feature_cols])\n\n# Create new column names for the principal components, e.g., PC1, PC2, ..., PC60\npc_columns = [f'PC{i+1}' for i in range(60)]\n\n# Create a new DataFrame containing the principal components and concatenate it with the original DataFrame\npca_train_df = pd.DataFrame(data=train_pca_result, columns=pc_columns)\npca_test_df = pd.DataFrame(data=test_pca_result, columns=pc_columns)\ntrain = pd.concat([train, pca_train_df], axis=1)\ntest = pd.concat([test, pca_test_df], axis=1)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:11.555938Z","iopub.execute_input":"2023-09-14T13:04:11.556558Z","iopub.status.idle":"2023-09-14T13:04:13.594620Z","shell.execute_reply.started":"2023-09-14T13:04:11.556522Z","shell.execute_reply":"2023-09-14T13:04:13.593684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Classes","metadata":{}},{"cell_type":"code","source":"class TrainDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:13.598931Z","iopub.execute_input":"2023-09-14T13:04:13.599410Z","iopub.status.idle":"2023-09-14T13:04:13.617244Z","shell.execute_reply.started":"2023-09-14T13:04:13.599365Z","shell.execute_reply":"2023-09-14T13:04:13.615595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"markdown","source":"#### Helper Functions","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    correct_preds = 0\n    total_samples = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        \n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()  # Allows you to control your model's LR according to some pre-set schedule or based on performance improvements\n        \n        # Calculate accuracy\n        predicted_labels = (outputs.sigmoid() > 0.5).float()\n        correct_preds += (predicted_labels == targets).sum().item()\n        total_samples += targets.size(0)\n        \n        final_loss += loss.item()\n    \n    accuracy = correct_preds / total_samples\n    final_loss /= len(dataloader)\n    \n    return final_loss, accuracy\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    correct_preds = 0\n    total_samples = 0\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        # Calculate accuracy\n        predicted_labels = (outputs.sigmoid() > 0.5).float()\n        correct_preds += (predicted_labels == targets).sum().item()\n        total_samples += targets.size(0)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    accuracy = correct_preds / total_samples\n    final_loss /= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, accuracy, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().cpu().numpy())  # Removed the unnecessary .detach()\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:13.618998Z","iopub.execute_input":"2023-09-14T13:04:13.619536Z","iopub.status.idle":"2023-09-14T13:04:13.637284Z","shell.execute_reply.started":"2023-09-14T13:04:13.619492Z","shell.execute_reply":"2023-09-14T13:04:13.636486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model\n* BatchNormalization: done between the layers of a NN instead of in the raw data. It serves to speed up training and use higher Learning rates making learning easier\n* If only tuning the CNN params was as easy as GBDT :)","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self, num_features, hidden_size):\n        super(Model, self).__init__()\n        \n        # Define channel sizes\n        channel_1 = 256\n        channel_2 = 512\n        channel_3 = 512\n\n        # Calculate reshape and pooling sizes\n        channel_1_reshape = int(hidden_size / channel_1)\n        channel_pool_1 = int(hidden_size / channel_1 / 2)\n        channel_pool_2 = int(hidden_size / channel_1 / 2 / 2) * channel_3\n\n        # Store values as instance variables\n        self.channel_1 = channel_1\n        self.channel_2 = channel_2\n        self.channel_3 = channel_3\n        self.channel_1_reshape = channel_1_reshape\n        self.channel_pool_1 = channel_pool_1\n        self.channel_pool_2 = channel_pool_2\n\n        # Layer 1: Batch normalization, dropout, and linear transformation\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.1)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n        # Layer 2: Convolutional layer with batch normalization and dropout\n        self.batch_norm_c1 = nn.BatchNorm1d(channel_1)\n        self.dropout_c1 = nn.Dropout(0.1)\n        self.conv1 = nn.utils.weight_norm(nn.Conv1d(channel_1, channel_2, kernel_size=5, stride=1, padding=2, bias=False), dim=None)\n\n        # Layer 3: Adaptive average pooling\n        self.ave_pool_c1 = nn.AdaptiveAvgPool1d(output_size=channel_pool_1)\n\n        # Layer 4: Convolutional layer with batch normalization and dropout\n        self.batch_norm_c2 = nn.BatchNorm1d(channel_2)\n        self.dropout_c2 = nn.Dropout(0.1)\n        self.conv2 = nn.utils.weight_norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True), dim=None)\n\n        # Layer 5: Additional convolutional layer with batch normalization and dropout\n        self.batch_norm_c2_1 = nn.BatchNorm1d(channel_2)\n        self.dropout_c2_1 = nn.Dropout(0.3)\n        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True), dim=None)\n\n        # Layer 6: Additional convolutional layer with batch normalization and dropout\n        self.batch_norm_c2_2 = nn.BatchNorm1d(channel_2)\n        self.dropout_c2_2 = nn.Dropout(0.2)\n        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True), dim=None)\n\n        # Layer 7: Max pooling\n        self.max_pool_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n        # Layer 8: Flatten\n        self.flatten = nn.Flatten()\n\n        # New Output Layer for Binary Classification\n        self.output_layer = nn.utils.weight_norm(nn.Linear(channel_pool_2, 1))\n\n    def forward(self, x):\n        # Layer 1: Batch normalization, dropout, and linear transformation\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.celu(self.dense1(x), alpha=0.06)\n\n        # Reshape\n        x = x.reshape(x.shape[0], self.channel_1, self.channel_1_reshape)\n\n        # Layer 2: Convolutional layer with batch normalization and dropout\n        x = self.batch_norm_c1(x)\n        x = self.dropout_c1(x)\n        x = F.relu(self.conv1(x))\n\n        # Layer 3: Adaptive average pooling\n        x = self.ave_pool_c1(x)\n\n        # Layer 4: Convolutional layer with batch normalization and dropout\n        x = self.batch_norm_c2(x)\n        x = self.dropout_c2(x)\n        x = F.relu(self.conv2(x))\n        x_s = x\n\n        # Layer 5: Additional convolutional layer with batch normalization and dropout\n        x = self.batch_norm_c2_1(x)\n        x = self.dropout_c2_1(x)\n        x = F.relu(self.conv2_1(x))\n\n        # Layer 6: Additional convolutional layer with batch normalization and dropout\n        x = self.batch_norm_c2_2(x)\n        x = self.dropout_c2_2(x)\n        x = F.relu(self.conv2_2(x))\n        x = x * x_s\n\n        # Layer 7: Max pooling\n        x = self.max_pool_c2(x)\n\n        # Layer 8: Flatten\n        x = self.flatten(x)\n\n        # New Output Layer with Sigmoid Activation for Binary Classification\n        logits = self.output_layer(x)\n        probabilities = torch.sigmoid(logits)\n\n        return probabilities  # Return probability scores\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:13.638826Z","iopub.execute_input":"2023-09-14T13:04:13.639689Z","iopub.status.idle":"2023-09-14T13:04:13.660452Z","shell.execute_reply.started":"2023-09-14T13:04:13.639652Z","shell.execute_reply":"2023-09-14T13:04:13.659846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setup / Prepare","metadata":{}},{"cell_type":"code","source":"feature_cols = [col for col in test.columns if 'PC' in col]\nnum_features=len(feature_cols)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:13.661798Z","iopub.execute_input":"2023-09-14T13:04:13.662445Z","iopub.status.idle":"2023-09-14T13:04:13.675136Z","shell.execute_reply.started":"2023-09-14T13:04:13.662407Z","shell.execute_reply":"2023-09-14T13:04:13.674208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Single Fold Training","metadata":{}},{"cell_type":"code","source":"def run_training(fold, seed):\n    seed_everything(seed)\n    trn_idx = train[train['fold'] != fold].index\n    val_idx = train[train['fold'] == fold].index\n    \n    train_df = train[train['fold'] != fold].reset_index(drop=True)\n    valid_df = train[train['fold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values  \n    \n    train_dataset = TrainDataset(x_train, y_train)\n    valid_dataset = TrainDataset(x_valid, y_valid)\n    \n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n            num_features = num_features,\n            hidden_size = hidden_size\n    )\n    \n    model.to(DEVICE)\n    optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n    \n    #schedulers\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.01, div_factor=1e3, max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,factor=0.1,patience=1,verbose=True)\n    #scheduler = optim.lr_scheduler.CyclicLR(optimizer=optimizer, base_lr = 1e-4, max_lr=1e-2,step_size_up=100,cycle_momentum=False,mode=\"triangular2\")\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), num_targets))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        train_loss, train_accuracy = train_fn(model, optimizer, scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}, train_accuracy: {train_accuracy}\")\n        valid_loss,valid_accuracy, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}, valid_accuracy: {valid_accuracy}\")\n        #scheduler.step(valid_loss) #delete\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break        \n                \n                \n# <<<<<<<<<<<---------------------------------PREDICTION------------------------------------------------------------>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n    x_test = test[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model = Model(\n        num_features=num_features,\n        hidden_size=hidden_size,\n    )\n\n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n\n    predictions = np.zeros((len(test), num_targets))\n    predictions = inference_fn(model, testloader, DEVICE)\n\n    return oof, predictions\n    \n        \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:13.678296Z","iopub.execute_input":"2023-09-14T13:04:13.678550Z","iopub.status.idle":"2023-09-14T13:04:13.694643Z","shell.execute_reply.started":"2023-09-14T13:04:13.678527Z","shell.execute_reply":"2023-09-14T13:04:13.693714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_k_folds(NFOLDS, seed):\n    oof = np.zeros((len(train), num_targets))\n    predictions = np.zeros((len(test), num_targets))\n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ / NFOLDS\n        oof += oof_\n        \n    return oof, predictions\n    ","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:13.695825Z","iopub.execute_input":"2023-09-14T13:04:13.696810Z","iopub.status.idle":"2023-09-14T13:04:13.709189Z","shell.execute_reply.started":"2023-09-14T13:04:13.696778Z","shell.execute_reply":"2023-09-14T13:04:13.708286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = [0, 1, 2]\noof = np.zeros((len(train), num_targets))\npredictions = np.zeros((len(test), num_targets))\n\nfor seed in SEED:\n    print(f\"Training for seed: {seed}\")\n    \n    oof_, predictions_ = run_k_folds(NFOLDS, seed)\n    oof += oof_ / len(SEED)\n    predictions += predictions_ / len(SEED)\n    \n    print(f\" Finished Training for seed: {seed}\")\n    print()\n          \n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T13:04:13.710656Z","iopub.execute_input":"2023-09-14T13:04:13.711015Z","iopub.status.idle":"2023-09-14T15:32:24.408016Z","shell.execute_reply.started":"2023-09-14T13:04:13.710962Z","shell.execute_reply":"2023-09-14T15:32:24.407007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(predictions), test.shape, len(oof), train.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:32:24.409414Z","iopub.execute_input":"2023-09-14T15:32:24.410053Z","iopub.status.idle":"2023-09-14T15:32:24.418407Z","shell.execute_reply.started":"2023-09-14T15:32:24.410016Z","shell.execute_reply":"2023-09-14T15:32:24.416410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['preds'] = oof[:, 1]\ntest['preds'] = predictions[:, 1]\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:32:24.419673Z","iopub.execute_input":"2023-09-14T15:32:24.421766Z","iopub.status.idle":"2023-09-14T15:32:24.431362Z","shell.execute_reply.started":"2023-09-14T15:32:24.421718Z","shell.execute_reply":"2023-09-14T15:32:24.430408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['preds'].describe()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:32:24.432799Z","iopub.execute_input":"2023-09-14T15:32:24.433166Z","iopub.status.idle":"2023-09-14T15:32:24.449880Z","shell.execute_reply.started":"2023-09-14T15:32:24.433133Z","shell.execute_reply":"2023-09-14T15:32:24.448958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ntrain['Label'] = np.where(train['preds']> 0.53, 1, 0)\naccuracy_score(train['Label'], train['Target'])","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:36:06.970014Z","iopub.execute_input":"2023-09-14T15:36:06.970390Z","iopub.status.idle":"2023-09-14T15:36:06.981953Z","shell.execute_reply.started":"2023-09-14T15:36:06.970361Z","shell.execute_reply":"2023-09-14T15:36:06.981002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Label'] = np.where(test['preds']> 0.53, 1,0)\ntest['Label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:36:54.979585Z","iopub.execute_input":"2023-09-14T15:36:54.980184Z","iopub.status.idle":"2023-09-14T15:36:54.988816Z","shell.execute_reply.started":"2023-09-14T15:36:54.980149Z","shell.execute_reply":"2023-09-14T15:36:54.987933Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = test[[\"ID\",\"Label\"]]\nsub.to_csv(\"cnn_approach.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2023-09-14T15:37:55.556320Z","iopub.execute_input":"2023-09-14T15:37:55.556683Z","iopub.status.idle":"2023-09-14T15:37:55.578721Z","shell.execute_reply.started":"2023-09-14T15:37:55.556653Z","shell.execute_reply":"2023-09-14T15:37:55.577794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}